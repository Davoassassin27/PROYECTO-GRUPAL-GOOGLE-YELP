{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Librerias a usar en el proceso de ETL**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install fuzz\n",
    "#pip install python-Levenshtein\n",
    "#!pip install fuzzywuzzy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "import pickle\n",
    "import ast\n",
    "from fuzzywuzzy import fuzz,process\n",
    "import functools\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1- ETL de Archivos YELP"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1- Archivo checkin.json"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Abrimos el archivo con la siguiente función:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Funcion para abrir chekin de yelp\n",
    "\n",
    "def abrir_Archivo_json(archivo):\n",
    "    merged_data = []  # Lista para almacenar los objetos JSON combinados\n",
    "\n",
    "    with open(archivo) as file:\n",
    "        for line in file:\n",
    "            try:\n",
    "                obj = json.loads(line)\n",
    "                merged_data.append(obj)\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"Error al decodificar JSON en el archivo {archivo}: {str(e)}\")\n",
    "\n",
    "    df = pd.DataFrame(merged_data)  # Crear DataFrame a partir de los objetos JSON\n",
    "    return df\n",
    "\n",
    "# Ejemplo de uso\n",
    "archivo = '../checkin.json'\n",
    "df_checkin_yelp = abrir_Archivo_json(archivo)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Revisamos si tiene nulos**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_checkin_yelp.isnull().sum()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Revisamos si tiene duplicados**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_checkin_yelp.duplicated().sum()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Desanidamos la fecha (en Fecha y hora)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "desanidados = []\n",
    "\n",
    "for _, row in df_checkin_yelp.iterrows():\n",
    "    business_id = row['business_id']\n",
    "    dates = row['date'].split(', ')\n",
    "    for date in dates:\n",
    "        time, date = date.split(' ')\n",
    "        desanidados.append([business_id, time, date])\n",
    "\n",
    "df_desanidado = pd.DataFrame(desanidados, columns=['business_id', 'hour', 'fecha'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Finalmente se exporta**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_desanidado.to_csv('checkin.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2- Archivo user.parquet"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Leemos el archivo**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "archivo = \"../user.parquet\"\n",
    "df_User_yelp = pd.read_parquet(archivo)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Eliminamos duplicados y reindexamos**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_User_yelp.drop_duplicates(subset=[\"user_id\", \"name\"], keep=\"first\")\n",
    "df_User_yelp = df_User_yelp.reindex(df.index)\n",
    "df_User_yelp['id_user'] = df.index"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Creamos y exportamos un nuevo dataframe para conservar id_user original con su user_id , el cual será usado para unir tablas**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfusuario = df_User_yelp.loc[:, [\"id_user\",'user_id']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Renombramos la columna user_id\n",
    "\n",
    "dfusuario.rename(columns={\"user_id\": \"yelp_id\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exportamos el nuevo dataframe llamado dfusuario\n",
    "dfusuario.to_csv(\"UsuarioYelp\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Eliminamos la columna user_id del dataframe df_user_yelp**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_User_yelp.drop(columns=['user_id'], inplace=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Aplicamos transformaciones a los tipos de datos en las diferentes variables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cambiamos a string la variable name\n",
    "\n",
    "df_User_yelp['name'] = df['name'].astype(str)\n",
    "\n",
    "#cambiamos a tipo datetime la variable yelping_since\n",
    "df_User_yelp['yelping_since'] = pd.to_datetime(df['yelping_since'])\n",
    "\n",
    "#Creamos una columna año a partir de elite que devuelva una lista de años separado por comas\n",
    "df_User_yelp['years'] = df['elite'].str.split(',')\n",
    "\n",
    "#creamos un nuevo DataFrame con filas individuales para cada año y usuario\n",
    "years_df = df_User_yelp[['id_user', 'years']].explode('years')\n",
    "\n",
    "#reemplazamos valores vacios por np.nan en years\n",
    "years_df['years'] = years_df['years'].replace(\"\", np.nan)\n",
    "\n",
    "#reemplazamos 20 po 2020 en years\n",
    "years_df['years'] = years_df['years'].replace(\"20\", \"2020\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exportamos segundo dataframe de user_yelp como usuarioElite**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "years_df.to_csv(\"Datasets/UsuarioElite\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Eliminamos las columnas elite y years del dataframe original user.parquet**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_User_yelp.drop(columns=['elite'], inplace=True)\n",
    "df_User_yelp.drop(columns=['years'], inplace=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Archivo business.pkl"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Leemos el archivo**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_business= pd.read_pickle('../business.pkl')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Agregamos nuevas columnas al dataframe con valores iniciales en NONE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_business['NAME']=None\n",
    "df_business['REVIEW_COUNT']=None\n",
    "df_business['POSTAL_CODE']=None\n",
    "df_business['CITY']=None\n",
    "df_business['STATE']=None\n",
    "df_business['BUSINESS_ID']=None\n",
    "df_business['ADDRESS']=None\n",
    "df_business['LATITUDE']=None\n",
    "df_business['LONGITUDE']=None\n",
    "df_business['STARS']=None\n",
    "df_business['IS_OPEN']=None\n",
    "df_business['ATTRIBUTES']=None\n",
    "df_business['CATEGORIES']=None\n",
    "df_business['HOURS']=None"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Obtenemos la actualización de la columna \"NAME\" del DataFrame df_business, donde se fusionan las cadenas de texto presentes en cada valor, eliminando los caracteres no-alfabéticos.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index,i in enumerate(df_business.name.values):\n",
    "    arr=[]\n",
    "    for e in i:\n",
    "        if isinstance(e,str):\n",
    "         arr.append(e)\n",
    "    df_business['NAME'][index]=''.join(arr)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Obtenemos la actualización de la columna \"CITY\" del DataFrame df_business, donde se fusionan las cadenas de texto presentes en cada valor, eliminando los caracteres no-alfabéticos.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index,i in enumerate(df_business.city.values):\n",
    "    arr=[]\n",
    "    for e in i:\n",
    "        if isinstance(e,str):\n",
    "         arr.append(e)\n",
    "    df_business['CITY'][index]=''.join(arr)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Obtenemos la actualización de la columna \"STATE\" del DataFrame df_business, donde se fusionan las cadenas de texto presentes en cada valor, eliminando los caracteres no-alfabéticos.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index,i in enumerate(df_business.state.values):\n",
    "    arr=[]\n",
    "    for e in i:\n",
    "        if isinstance(e,str):\n",
    "         arr.append(e)\n",
    "    df_business['STATE'][index]=''.join(arr)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**En este script, se actualiza la columna 'REVIEW_COUNT' del DataFrame df_business con el primer número entero encontrado en cada valor, recorriendo los valores de la columna y almacenando los números enteros en una lista antes de asignarlos a la columna.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index,i in enumerate(df_business.review_count.values):\n",
    "    arr=[]\n",
    "    for e in i:\n",
    "        if isinstance(e,int):\n",
    "         arr.append(e)\n",
    "    df_business['REVIEW_COUNT'][index]=arr[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Se fusionan los caracteres de texto presentes en cada valor de la columna 'business_id' del DataFrame df_b, actualizando la columna \"BUSINESS_ID\" con los valores resultantes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index,i in enumerate(df_business.business_id.values):\n",
    "    arr=[]\n",
    "    for e in i:\n",
    "        if isinstance(e,str):\n",
    "         arr.append(e)\n",
    "    df_business['BUSINESS_ID'][index]=''.join(arr)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Se fusionan los caracteres de texto presentes en cada valor de la columna 'address' del DataFrame 'df_business', actualizando la columna 'ADDRESS' con los valores resultantes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index,i in enumerate(df_business.address.values):\n",
    "    arr=[]\n",
    "    for e in i:\n",
    "        if isinstance(e,str):\n",
    "         arr.append(e)\n",
    "    df_business['ADDRESS'][index]=''.join(arr)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Se fusionan los caracteres de texto presentes en cada valor de la columna 'postal_code' del DataFrame 'df_business', actualizando la columna 'POSTAL_CODE' con los valores resultantes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index,i in enumerate(df_business.postal_code.values):\n",
    "    arr=[]\n",
    "    for e in i:\n",
    "        if isinstance(e,str):\n",
    "         arr.append(e)\n",
    "    df_business['POSTAL_CODE'][index]=''.join(arr)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Se filtran los elementos numéricos mayores que 1 en cada valor de la columna 'latitude' del DataFrame 'df_business'. A continuación, se asigna el primer elemento filtrado a la columna 'LATITUDE' en el DataFrame 'df_business' en la fila correspondiente al índice actual.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index,i in enumerate(df_business.latitude.values):\n",
    "    arr=[]\n",
    "    for e in i:\n",
    "       if e>1:\n",
    "         arr.append(e)\n",
    "    df_business['LATITUDE'][index]=arr[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Se filtran los elementos numéricos menores que -1 en cada valor de la columna 'longitude' del DataFrame 'df_business'. Luego, se asigna el primer elemento filtrado a la columna 'LONGITUDE' en el DataFrame 'df_business' en la fila correspondiente al índice actual.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index,i in enumerate(df_business.longitude.values):\n",
    "    arr=[]\n",
    "    for e in i:\n",
    "        if e<-1:\n",
    "            arr.append(e)\n",
    "    df_business['LONGITUDE'][index]=arr[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Se filtran los elementos numéricos mayores que 0.1 en cada valor de la columna 'stars' del DataFrame 'df_business'. Después, se asigna el primer elemento filtrado a la columna 'STARS' en el DataFrame 'df_business' en la fila correspondiente al índice actual.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index,i in enumerate(df_business.stars.values):\n",
    "    arr=[]\n",
    "    for e in i:\n",
    "       if e>0.1:\n",
    "         arr.append(e)\n",
    "    df_business['STARS'][index]=arr[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Se filtran los elementos numéricos mayores o iguales a 0 en cada valor de la columna 'is_open' del DataFrame 'df_business'. A continuación, se asigna el primer elemento filtrado a la columna 'IS_OPEN' en el DataFrame 'df_business' en la fila correspondiente al índice actual**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index,i in enumerate(df_business.is_open.values):\n",
    "    arr=[]\n",
    "    for e in i:\n",
    "       if e >=0:\n",
    "         arr.append(e)\n",
    "    df_business['IS_OPEN'][index]=arr[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Este código busca y guarda el primer diccionario encontrado en la columna 'attributes' del dataframe 'df_business' en la columna 'ATTRIBUTES'.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index,i in enumerate(df_business.attributes.values):\n",
    "    arr=[]\n",
    "    for e in i:\n",
    "        if isinstance(e,dict):\n",
    "         arr.append(e)\n",
    "    if len(arr)>0:\n",
    "     df_business['ATTRIBUTES'][index]=arr[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Este código filtra los elementos de tipo cadena en la columna 'categories' del dataframe 'df_business'. Luego, fusiona todas las cadenas filtradas en una sola cadena y la asigna a la columna 'CATEGORIES' en el dataframe 'df_business'.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index,i in enumerate(df_business.categories.values):\n",
    "    arr=[]\n",
    "    for e in i:\n",
    "        if isinstance(e,str):\n",
    "         arr.append(e)\n",
    "    df_business['CATEGORIES'][index]=''.join(arr)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**En este código, se recorren los valores de la columna 'hours' en el dataframe 'df_business' y se filtran los elementos que son diccionarios. Luego, se asigna el primer diccionario encontrado a la columna 'HOURS' en el dataframe 'df_business' en la fila correspondiente. En resumen, el código extrae y guarda el primer diccionario encontrado en la columna 'hours' del dataframe 'df_business' en la columna 'HOURS'.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index,i in enumerate(df_business.hours.values):\n",
    "    arr=[]\n",
    "    for e in i:\n",
    "        if isinstance(e,dict):\n",
    "         arr.append(e)\n",
    "    if len(arr)>0:     \n",
    "     df_business['HOURS'][index]=arr[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Filtramos el dataframe solo a las columnas nuevas generadas con los bucles, con los nombres iniciales**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_business=df_business[['BUSINESS_ID','NAME','REVIEW_COUNT','CITY','STATE','ADDRESS','POSTAL_CODE','LATITUDE','LONGITUDE','STARS','IS_OPEN','ATTRIBUTES','CATEGORIES','HOURS']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_business['business_id']=df_business['BUSINESS_ID']\n",
    "df_business['name']=df_business['NAME']\n",
    "df_business['address']=df_business['NAME']\n",
    "df_business['city']=df_business['CITY']\n",
    "df_business['state']=df_business['STATE']\n",
    "df_business['postal_code']=df_business['POSTAL_CODE']\n",
    "df_business['latitude']=df_business['LATITUDE']\n",
    "df_business['longitude']=df_business['LONGITUDE']\n",
    "df_business['stars']=df_business['STARS']\n",
    "df_business['review_count']=df_business['REVIEW_COUNT']\n",
    "df_business['is_open']=df_business['IS_OPEN']\n",
    "df_business['attributes']=df_business['ATTRIBUTES']\n",
    "df_business['categories']=df_business['CATEGORIES']\n",
    "df_business['hours']=df_business['HOURS']\n",
    "df_business=df_business[['business_id', 'name', 'address', 'city', 'state', 'postal_code',\n",
    "       'latitude', 'longitude', 'stars', 'review_count', 'is_open',\n",
    "       'attributes', 'categories', 'hours']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_business2 = df_business.copy()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cargamos un nuevo csv con las ciudades de estados unidos**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "city=pd.read_csv('../ciudades.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ciudades unicas de estados unidos\n",
    "ciudades_estados_unidos = city['City'].unique()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**la primera función busca la mejor coincidencia de una ciudad en una lista, mientras que la segunda función calcula un umbral de similitud basado en la longitud de una ciudad. Ambas funciones se utilizan en conjunto para determinar la mejor coincidencia de una ciudad y aplicar un criterio de aceptación basado en el umbral de similitud.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "\n",
    "@functools.lru_cache(maxsize=None)  \n",
    "def encontrar_mejor_coincidencia(ciudad):\n",
    "    mejor_coincidencia = process.extractOne(ciudad, ciudades_estados_unidos)\n",
    "    resultado = mejor_coincidencia[0] if mejor_coincidencia[1] >= calcular_umbral_similitud(len(ciudad)) else ciudad\n",
    "    return resultado\n",
    "\n",
    "def calcular_umbral_similitud(longitud_ciudad):\n",
    "    umbral_base = 55\n",
    "    umbral = umbral_base - (longitud_ciudad // 3)\n",
    "    return max(umbral, umbral_base)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Se aplica la función encontrar_mejor_coincidencia a cada valor de la columna 'city' del DataFrame df_b. La función busca la mejor coincidencia de cada ciudad en una lista de ciudades de EE.UU. y actualiza la columna 'city' con las mejores coincidencias encontradas.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_business2['city']=df_business2['city'].apply(encontrar_mejor_coincidencia)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#probamos la funcion\n",
    "encontrar_mejor_coincidencia('nryork')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Generamos un dataframe desde city con las columnas City y State solamente**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "citystate=city[['City','State']]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Se fusionan dos DataFrames ('df_business' y 'citystate') por la columna 'city' en una unión izquierda.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_business2=df_business2.merge(citystate,left_on='city',right_on='City',how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminar la columna duplicada 'state'\n",
    "df_business2.drop('state', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_business2.head(1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**se actualiza el dataframe df_business, manteniendo solo las columnas mencionadas.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_business2=df_business2[['business_id', 'name', 'address', 'city', 'State',\n",
    "       'postal_code', 'latitude', 'longitude', 'stars', 'review_count',\n",
    "       'is_open', 'attributes', 'categories', 'hours']]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Renombramos la columna State a state**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_business2=df_business2.rename(columns={'State':'state'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_business2.head(1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Filtramos el dataframe con los 5 estados que se usarán en el proyecto**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "statein=['California','New York' ,'Florida' ,'Pennsylvania' ,'Texas']\n",
    "df_business2=df_business2[df_business2['state'].isin(statein)].reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_business2.head(1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Filtramos las columnas que se van a usar en el proyecto y renombramos index**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_business2=df_business2[['index','business_id', 'name', 'address', 'city',\n",
    "       'state', 'postal_code', 'latitude', 'longitude', 'stars',\n",
    "       'review_count', 'is_open', 'attributes', 'categories', 'hours']]\n",
    "df_business2=df_business2.rename(columns={'index':'id_business'})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Generamos un nuevo dataframe solo con las columnas 'id_business','business_id'**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creamos la tabla de dimension BusinessYelp\n",
    "BusinessYelp = df_business2[['id_business', 'business_id']]\n",
    "\n",
    "# Renombrar la columna \"business_id\" como \"businessYelp_id\"\n",
    "BusinessYelp.rename(columns={\"business_id\": \"businessYelp_id\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BusinessYelp.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exportamos la tabla\n",
    "BusinessYelp.to_csv('BusinessYelpId.csv',index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Filtramos por ultima vez el dataframe**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_business2=df_business2[['id_business','name', 'address', 'city', 'state',\n",
    "       'postal_code', 'latitude', 'longitude', 'stars', 'review_count',\n",
    "       'is_open', 'attributes', 'categories', 'hours']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_business2.head(1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Convertimos a númerico la columna postal_code**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_business2[\"postal_code\"] = df_business2[\"postal_code\"].apply(pd.to_numeric, errors='coerce')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Creamos un dataframe auxiliar  que contiene solo las filas donde el valor de la columna 'state' coincide con \"Pennsylvania\", \"New York\", \"Florida\", \"California\" o \"Texas\".**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aux = df_business2.query('state == \"Pennsylvania\" or state == \"New York\" or state == \"Florida\" or state == \"California\" or state == \"Texas\"')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Buscamos filas específicas en el DataFrame y realizamos cambios en las columnas 'state' y 'postal_code' en función de los valores de las columnas 'city' y 'postal_code'.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_business2.loc[(df_business2[\"city\"] == \"Philadelphia\") & (df_business2[\"postal_code\"].isnull()), \"state\"] = \"Pennsylvania\"\n",
    "df_business2.loc[(df_business2[\"city\"] == \"Philadelphia\") & (df_business2[\"postal_code\"].isnull()), \"postal_code\"] = 19107\n",
    "df_business2.loc[df_business2[\"city\"] == \"Saint Petersburg\", \"city\"] = \"St. Petersburg\"\n",
    "df_business2.loc[(df_business2[\"city\"] == \"St. Petersburg\") & (df_business2[\"postal_code\"].isnull()), \"state\"] = \"Florida\"\n",
    "df_business2.loc[(df_business2[\"city\"] == \"St. Petersburg\") & (df_business2[\"postal_code\"].isnull()), \"postal_code\"] = 33707\n",
    "df_business2.loc[(df_business2[\"city\"] == \"Tampa\") & (df_business2[\"postal_code\"].isnull()), \"state\"] = \"Florida\"\n",
    "df_business2.loc[(df_business2[\"city\"] == \"Tampa\") & (df_business2[\"postal_code\"].isnull()), \"postal_code\"] = 33610\n",
    "df_business2.loc[(df_business2[\"city\"] == \"Santa Barbara\") & (df_business2[\"postal_code\"].isnull()), \"state\"] = \"California\"\n",
    "df_business2.loc[(df_business2[\"city\"] == \"Santa Barbara\") & (df_business2[\"postal_code\"].isnull()), \"postal_code\"] = 93101\n",
    "df_business2.loc[(df_business2[\"city\"] == \"Pasco\") & (df_business2[\"postal_code\"].isnull()), \"state\"] = \"Florida\"\n",
    "df_business2.loc[(df_business2[\"city\"] == \"Pasco\") & (df_business2[\"postal_code\"].isnull()), \"postal_code\"] = 33544"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exportamos df_business2 para el proyecto**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_business2.to_csv('Business.csv',index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Eliminamos algunas columnas del dataframe auxiliar**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aux.drop(columns=['is_open'], inplace=True)\n",
    "aux.drop(columns=['postal_code'], inplace=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Eliminamos duplicados de la columna auxiliar y reindexamos**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aux = aux.drop_duplicates(subset=[\"id_business\", \"name\"], keep=\"first\")\n",
    "aux['id_business'] = aux.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aux.columns"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Generamos un nuevo dataframe con \"id_business\",'categories'**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tabla de dimension de categorias\n",
    "df_cat = aux.loc[:, ['id_business','categories']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividir los valores de la columna \"categories\" por coma y expandirlos en filas\n",
    "df_cat['categories'] = df_cat['categories'].str.split(',')\n",
    "df_cat = df_cat[['id_business', 'categories']].explode('categories')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creamos un array con las categorias unicas\n",
    "categorias = df_cat[\"categories\"].unique()\n",
    "#Creamos la tabla de dimension de categorias de yelp\n",
    "df_categorias = pd.DataFrame(categorias, columns=[\"Descripcion\"])\n",
    "df_categorias['IdCategoria'] = df_categorias.index"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exportamos la tabla de dimensiones de categorias**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_categorias.to_csv(\"Categorias.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Creamos una nuevo dataframe auxiliar para el detalle de las categorias a partir de df_Cat y df_categorias**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_aux = df_cat.merge(df_categorias, left_on=\"categories\", right_on=\"Descripcion\", how=\"inner\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Eliminamos columnas categories y Descripcion para generar una tabla de dimension entre id_business y idcategoria**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_aux.drop(columns=[\"categories\"],inplace=True)\n",
    "df_aux.drop(columns=[\"Descripcion\"],inplace=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exportamos tabla de dimensiones de detalle de categorias**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_aux.to_csv(\"DetalleCategorias.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aux.head(1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Eliminamos categories de aux**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aux.drop(columns=[\"categories\"],inplace=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Generamos tabla de dimension atributos a partir de aux**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_atribute = aux.loc[:, [\"id_business\",'attributes']]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Procesamos y dividimos los elementos de la columna 'attributes'**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, i in enumerate(df_atribute['attributes']):\n",
    "    if isinstance(i,str):\n",
    "       df_atribute['attributes'][index]= i[1:-1].split(',')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expandimos la columna 'attributes' del DataFrame 'df' en filas separadas, manteniendo los valores correspondientes de la columna 'id_business'.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_atribute = df_atribute[['id_business', 'attributes']].explode('attributes')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Eliminamos los corchetes y comillas dobles de la columna 'attributes' del DataFrame, dejando los valores limpios y sin esos caracteres específicos**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_atribute['attributes'] = df_atribute['attributes'].str.replace('{', '').str.replace('}', '')\n",
    "df_atribute['attributes'] = df_atribute['attributes'].str.replace('\"', '')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**modificamos los valores en la columna 'attributes', actualizando ciertos aspectos relacionados con estacionamiento de negocios**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_atribute['attributes'] = df_atribute['attributes'].str.replace('street: True', 'BusinessParking street: True')\n",
    "df_atribute['attributes'] = df_atribute['attributes'].str.replace('street: False', 'BusinessParking street: False')\n",
    "df_atribute['attributes'] = df_atribute['attributes'].str.replace('validated: True', 'BusinessParking validated: True')\n",
    "df_atribute['attributes'] = df_atribute['attributes'].str.replace('validated: False', 'BusinessParking validated: False')\n",
    "df_atribute['attributes'] = df_atribute['attributes'].str.replace('lot: True', 'BusinessParking lot: True')\n",
    "df_atribute['attributes'] = df_atribute['attributes'].str.replace('lot: False', 'BusinessParking lot: False')\n",
    "df_atribute['attributes'] = df_atribute['attributes'].str.replace('valet: True', 'BusinessParking valet: True')\n",
    "df_atribute['attributes'] = df_atribute['attributes'].str.replace('valet: False', 'BusinessParking valet: False')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Creamos un nuevo DataFrame llamado 'df_atributos' que contiene los valores únicos de la columna 'attributes' del DataFrame original 'df_atribute', junto con una columna de identificación única para cada valor.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "atributos = df_atribute[\"attributes\"].unique()\n",
    "df_atributos = pd.DataFrame(atributos, columns=[\"Descripcion\"])\n",
    "df_atributos['IdAtributos'] = df_atributos.index"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exportamos tabla de atributos**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_atributos.to_csv(\"Datasets/Businessatributos.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Eliminamos las columnas \"attributes\" de aux**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aux.drop(columns=[\"attributes\"],inplace=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Creamos la tabla de dimensiones \"Hours\" a partir de aux**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hours = aux.loc[:, [\"id_business\",'hours']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hours"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exportamos la tabla de Hours**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hours.to_csv(\"Datasets/BusinessHoras.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Creamos el dataframe df_Hour_detalle**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hour_detalle=df_b[['id_business','hours']]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Desglosamos los diferentes horarios en diferentes filas para cada valor de forma individual**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hour_detalle=df_hour_Detalle[['id_business','hours']].explode('hours')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Creamos un dataframe llamado hour con las horas unicas de df_hour_detalle**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hours=df_hour_detalle['hours'].unique()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Se crea un nuevo DataFrame que contiene una sola columna llamada \"Descripcion\" que contiene los valores de la columna 'hours' y reindexamos**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DescHour=pd.DataFrame(hours,columns=[\"Descripcion\"])\n",
    "DescHour['id_hour']=DescHour.index"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exportamos el dataframe BusinessHorarios**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DescHour.to_csv('businessHorarios.csv',index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hacemos una union entre df_hour_detalle y DescHour**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hour_detalle.merge(DescHour,left_on='hours',right_on='Descripcion',how='inner')[['id_business','id_hour']]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exportamos el dataframe df_hour_detalle**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hour_detalle.to_csv('businessDetallesHora.csv',index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Creamos el dataframe df_detalleatributo a partir de la union entre df_atribute y df_atributos**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_detalleatributo = df_atribute.merge(df_atributos, left_on=\"attributes\", right_on=\"Descripcion\", how=\"inner\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Eliminamos las columnas \"attributes\" y \"Descripcion\" de df_detalleatributo**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_detalleatributo.drop(columns = \"attributes\",inplace=True)\n",
    "df_detalleatributo.drop(columns = \"Descripcion\",inplace=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exportamos dataframe df_detalleatributo**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfdetalleatributo.to_csv(\"Datasets/DetalleAtributos.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Eliminamos la columna hours de aux**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aux.drop(columns=[\"hours\"],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aux.head(1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4-tip.json"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Abrimos el archivo json**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_objects=[]\n",
    "\n",
    "with open('../tip.json', 'r',encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        json_objects.append(json.loads(line))\n",
    "\n",
    "\n",
    "df_tip = pd.DataFrame(json_objects)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " **Obtenemos un DataFrame que contenga solo las filas correspondientes a los negocios presentes en BusinessYelp**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tip=df_tip[df_tip['business_id'].isin(BusinessYelp.business_id.unique().tolist())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>business_id</th>\n",
       "      <th>text</th>\n",
       "      <th>date</th>\n",
       "      <th>compliment_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [user_id, business_id, text, date, compliment_count]\n",
       "Index: []"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tip"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hacemos una unión df_tip con BusinessYelp**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tip=BusinessYelp.merge(df_tip,left_on='business_id',right_on='business_id',how='right')[['user_id', 'id_business', 'text', 'date', 'compliment_count']]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hacemos una unión df_tip con UsuarioYelp**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tip=dfusuario.merge(df_tip,left_on='yelp_id',right_on='user_id',how='right')[['id_user', 'id_business', 'text', 'date', 'compliment_count']]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exportamos dataframe tip.json**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tip.to_csv('tipsYelp.csv',index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5-review.json"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Leemos el archivo review.json de Yelp**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jsonarr=[]\n",
    "with open(\"../review.json\", 'r',encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        jsonarr.append(json.loads(line))\n",
    "\n",
    "\n",
    "df_review = pd.DataFrame(jsonarr)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Eliminamos duplicados y reindexamos**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_review= df_review.drop_duplicates(subset=[\"review_id\", \"text\"], keep=\"first\")\n",
    "df_review['id_review'] = df_review.index"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Creamos un dataframe con \"id_review\", \"review_id\" llamado df_ReviewYelp**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ReviewYelp = df_review[[\"id_review\", \"review_id\"]]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exportamos el dataframe ReviewYelpId**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfReviewYelp.to_csv(\"Datasets/ReviewYelpId.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Eliminamos columna review_id de df_Review**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_review.drop(columns = \"review_id\",inplace=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Unimos el dataframe df_review con dfusuario**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_review = df_review.merge(dfusuario, left_on=\"user_id\", right_on=\"yelp_id\", how=\"inner\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Creamos el dataframe df_reviewfinal a partir de df_review y BusinessYelp**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reviewfinal = df_review.merge(BusinessYelp, left_on=\"business_id\", right_on=\"businessYelp_id\", how=\"inner\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Se eliminan las columnas \"user_id\" y \"yelp_id\" del dataframe df_reviewfinal**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reviewfinal.drop(columns = \"business_id\",inplace=True)\n",
    "df_reviewfinal.drop(columns = \"businessYelp_id\",inplace=True)\n",
    "df_reviewfinal.drop(columns = \"Yelp_id\",inplace=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exportamos dataframe review**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reviewfinal.to_csv(\"Review.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2-ETL de Archivos de Google Maps"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1-Review Estados"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Leemos el archivo del Estado de California**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Funcion para leer el estado de California\n",
    "def read_json_files(folder_path):\n",
    "    data = []\n",
    "    for file_name in os.listdir(folder_path):\n",
    "        if file_name.endswith(\".json\"):\n",
    "            file_path = os.path.join(folder_path, file_name)\n",
    "            with open(file_path, 'r') as file:\n",
    "                for line in file:\n",
    "                    line = line.strip()\n",
    "                    if line:\n",
    "                        try:\n",
    "                            json_obj = json.loads(line)\n",
    "                            data.append(json_obj)\n",
    "                        except json.JSONDecodeError:\n",
    "                            continue\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Leemos cada json y lo convertimos en un dataframe y lo agregamos a una lista de dataframe\n",
    "folder_path = \"../reviews-estados/review-California\"\n",
    "df_list = []\n",
    "df_california = read_json_files(folder_path)\n",
    "df_list.append(df_california)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#unimos todos los dataframes generados en uno\n",
    "df_california_consolidado = pd.concat(df_list, ignore_index=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Eliminamos columnas que no se usarán en el proyecto**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dropeamos las columnas que no usaremos\n",
    "df_california_consolidado.drop([\"name\", \"pics\", \"resp\"], axis=1, inplace=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cambiamos el formato de la columna time**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertir la columna \"time\" al formato adecuado\n",
    "df_california_consolidado['time'] = pd.to_datetime(df_california_consolidado['time'], unit='ms')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Creamos las columnas date y hour**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creamos dos columnas (date y hour) para ser extraidos de la columna time y convertir al formato adecuado \n",
    "from datetime import datetime\n",
    "df_california_consolidado['hour'] = pd.to_datetime(df_california_consolidado['time']).dt.strftime('%H:%M:%S')\n",
    "\n",
    "df_california_consolidado['date'] = pd.to_datetime(df_california_consolidado['time']).dt.strftime('%Y-%m-%d')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Se exporta el dataframe de California**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exportar el DataFrame\n",
    "df_california_consolidado.to_csv('california.csv', escapechar='\\\\', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**El mismo ETL se aplica a los estados de Florida, New York, Pensylvania y Texas**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2- Carpeta Metadata_Sitios"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Leemos la carpeta y la consolidamos en un archivo**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Funcion para abrir carpeta de metadata-sitios\n",
    "def merge_json_files(folder_path):\n",
    "    merged_data = []  # Lista para almacenar los objetos JSON combinados\n",
    "\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith('.json'):\n",
    "            filepath = os.path.join(folder_path, filename)\n",
    "            with open(filepath) as file:\n",
    "                for line in file:\n",
    "                    try:\n",
    "                        obj = json.loads(line)\n",
    "                        merged_data.append(obj)\n",
    "                    except json.JSONDecodeError as e:\n",
    "                        print(f\"Error al decodificar JSON en el archivo {filename}: {str(e)}\")\n",
    "\n",
    "    df = pd.DataFrame(merged_data)  # Crear DataFrame a partir de los objetos JSON\n",
    "    return df\n",
    "\n",
    "# Ejemplo de uso\n",
    "folder_path = '../metadata-sitios/'\n",
    "Metadata= merge_json_files(folder_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Abrimos el archivo del estado de california**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_json_files(folder_path):\n",
    "    merged_data = []  # Lista para almacenar los objetos JSON combinados\n",
    "\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith('.json'):\n",
    "            filepath = os.path.join(folder_path, filename)\n",
    "            with open(filepath) as file:\n",
    "                for line in file:\n",
    "                    try:\n",
    "                        obj = json.loads(line)\n",
    "                        merged_data.append(obj)\n",
    "                    except json.JSONDecodeError as e:\n",
    "                        print(f\"Error al decodificar JSON en el archivo {filename}: {str(e)}\")\n",
    "\n",
    "    df = pd.DataFrame(merged_data)  # Crear DataFrame a partir de los objetos JSON\n",
    "    return df\n",
    "\n",
    "# Ejemplo de uso\n",
    "folder_path = '../reviews-estados/review-California'\n",
    "df_california= merge_json_files(folder_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Filtramos Metadata_Sitios con el estado de california**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtrar los registros de Metadata que tienen el mismo 'gmap_id' que los de CaliforniaR\n",
    "MetaCalifornia = Metadata[Metadata['gmap_id'].isin(df_california['gmap_id'])].copy()\n",
    "\n",
    "# Agregar la nueva columna 'California' al nuevo DataFrame\n",
    "MetaCalifornia['State'] = 'California'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Eliminamos Filas con valores nulos**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminar filas con valores nulos en la columna 'category'\n",
    "MetaCalifornia = MetaCalifornia.dropna(subset=['category'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Abrimos el archivo para el estado de Florida**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_json_files(folder_path):\n",
    "    merged_data = []  # Lista para almacenar los objetos JSON combinados\n",
    "\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith('.json'):\n",
    "            filepath = os.path.join(folder_path, filename)\n",
    "            with open(filepath) as file:\n",
    "                for line in file:\n",
    "                    try:\n",
    "                        obj = json.loads(line)\n",
    "                        merged_data.append(obj)\n",
    "                    except json.JSONDecodeError as e:\n",
    "                        print(f\"Error al decodificar JSON en el archivo {filename}: {str(e)}\")\n",
    "\n",
    "    df = pd.DataFrame(merged_data)  # Crear DataFrame a partir de los objetos JSON\n",
    "    return df\n",
    "\n",
    "# Ejemplo de uso\n",
    "folder_path = '../reviews-estados/review-Florida/'\n",
    "df_florida= merge_json_files(folder_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Filtramos Metadata_Sitios con el estado de florida**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtrar los registros de Metadata que tienen el mismo 'gmap_id' que los de CaliforniaR\n",
    "MetaFlorida= Metadata[Metadata['gmap_id'].isin(df_florida['gmap_id'])].copy()\n",
    "\n",
    "# Agregar la nueva columna 'California' al nuevo DataFrame\n",
    "MetaFlorida['State'] = 'Florida'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Eliminamos Filas con valores nulos**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminar filas con valores nulos en la columna 'category'\n",
    "MetaFlorida = MetaFlorida.dropna(subset=['category'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Abrimos archivo de New york**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_json_files(folder_path):\n",
    "    merged_data = []  # Lista para almacenar los objetos JSON combinados\n",
    "\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith('.json'):\n",
    "            filepath = os.path.join(folder_path, filename)\n",
    "            with open(filepath) as file:\n",
    "                for line in file:\n",
    "                    try:\n",
    "                        obj = json.loads(line)\n",
    "                        merged_data.append(obj)\n",
    "                    except json.JSONDecodeError as e:\n",
    "                        print(f\"Error al decodificar JSON en el archivo {filename}: {str(e)}\")\n",
    "\n",
    "    df = pd.DataFrame(merged_data)  # Crear DataFrame a partir de los objetos JSON\n",
    "    return df\n",
    "\n",
    "# Ejemplo de uso\n",
    "folder_path = '../reviews-estados/review-New_York'\n",
    "df_new_york= merge_json_files(folder_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Filtramos Metadata_Sitios con el estado de New york**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtrar los registros de Metadata que tienen el mismo 'gmap_id' que los de CaliforniaR\n",
    "MetaNewYork= Metadata[Metadata['gmap_id'].isin(df_new_york['gmap_id'])].copy()\n",
    "\n",
    "# Agregar la nueva columna 'California' al nuevo DataFrame\n",
    "MetaNewYork['State'] = 'New York'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Eliminamos Filas con valores nulos**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminar filas con valores nulos en la columna 'category'\n",
    "MetaNewYork = MetaNewYork.dropna(subset=['category'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Abrimos archivo de Texas**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_json_files(folder_path):\n",
    "    merged_data = []  # Lista para almacenar los objetos JSON combinados\n",
    "\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith('.json'):\n",
    "            filepath = os.path.join(folder_path, filename)\n",
    "            with open(filepath) as file:\n",
    "                for line in file:\n",
    "                    try:\n",
    "                        obj = json.loads(line)\n",
    "                        merged_data.append(obj)\n",
    "                    except json.JSONDecodeError as e:\n",
    "                        print(f\"Error al decodificar JSON en el archivo {filename}: {str(e)}\")\n",
    "\n",
    "    df = pd.DataFrame(merged_data)  # Crear DataFrame a partir de los objetos JSON\n",
    "    return df\n",
    "\n",
    "# Ejemplo de uso\n",
    "folder_path = \"../reviews-estados/review-Texas\"\n",
    "df_Texas= merge_json_files(folder_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Filtramos Metadata_Sitios con el estado de New york**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtrar los registros de Metadata que tienen el mismo 'gmap_id' que los de CaliforniaR\n",
    "MetaTexas= Metadata[Metadata['gmap_id'].isin(df_Texas['gmap_id'])].copy()\n",
    "\n",
    "# Agregar la nueva columna 'California' al nuevo DataFrame\n",
    "MetaTexas['State'] = 'Texas'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Eliminamos Filas con valores nulos**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminar filas con valores nulos en la columna 'category'\n",
    "MetaTexas = MetaTexas.dropna(subset=['category'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Abrimos archivo de Pensylvania**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_json_files(folder_path):\n",
    "    merged_data = []  # Lista para almacenar los objetos JSON combinados\n",
    "\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith('.json'):\n",
    "            filepath = os.path.join(folder_path, filename)\n",
    "            with open(filepath) as file:\n",
    "                for line in file:\n",
    "                    try:\n",
    "                        obj = json.loads(line)\n",
    "                        merged_data.append(obj)\n",
    "                    except json.JSONDecodeError as e:\n",
    "                        print(f\"Error al decodificar JSON en el archivo {filename}: {str(e)}\")\n",
    "\n",
    "    df = pd.DataFrame(merged_data)  # Crear DataFrame a partir de los objetos JSON\n",
    "    return df\n",
    "\n",
    "# Ejemplo de uso\n",
    "folder_path = \"../reviews-estados/review-Pennsylvania\"\n",
    "df_Pensylvania= merge_json_files(folder_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Filtramos Metadata_Sitios con el estado de Pensylvania**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtrar los registros de Metadata que tienen el mismo 'gmap_id' que los de CaliforniaR\n",
    "MetaPensylvania= Metadata[Metadata['gmap_id'].isin(df_Pensylvania['gmap_id'])].copy()\n",
    "\n",
    "# Agregar la nueva columna 'California' al nuevo DataFrame\n",
    "MetaPensylvania['State'] = 'Pennsylvania'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Eliminamos Filas con valores nulos**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminar filas con valores nulos en la columna 'category'\n",
    "MetaPensylvania = MetaPensylvania.dropna(subset=['category'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Concatenamos todos los estados**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lista de los DataFrames \n",
    "dataframes = [MetaCalifornia, MetaFlorida, MetaNewYork, MetaTexas, MetaPensylvania]\n",
    "\n",
    "MetaEstados = pd.concat(dataframes)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Eliminamos las columnas \"Price\" y \"description\"**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "MetaEstados = MetaEstados.drop(['description', 'price'], axis=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exportamos el dataframe**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exportar el DataFrame\n",
    "MetaEstados.to_csv('MetaEstados.csv', escapechar='\\\\', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
